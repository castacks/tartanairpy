.. tartanair documentation master file, created by
   sphinx-quickstart on Wed Mar  1 21:13:43 2023.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

TartanAir Dataset Documentation
==========================================================

Install:

.. code-block:: bash

   pip install tartanair


Welcome to TartanAir V2! 

Let's go on an adventure to beautiful mountains, to dark caves, to stylish homes, to the Moon ðŸš€, and to other exciting places. And there is more! You, your models, and your robots can experience these worlds via a variety of sensors: LiDAR, IMU, optical cameras with any lense configuration you want (we provide customizable fisheye, pinhole, and equirectangular camera models), depth cameras, segmentation "cameras", and event cameras.

.. .. image:: images/title.png
..     :alt: TartanAir-V2
..     :target: https://www.youtube.com/watch?v=0BL8BPhlpTs

..  youtube:: 0BL8BPhlpTs

All the environments have recorded trajectories that were designed to be challenging and realistic. Can we improve the state of the art in SLAM, navigation, and robotics? 

ðŸ†• `TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation <tartanground/index.html>`_  

Useful Links
==========================================================
`TartanAir V1 <https://theairlab.org/tartanair-dataset/>`_

`TartanAir V2 Git Repo <https://github.com/castacks/tartanairpy>`_



.. toctree::
   :maxdepth: 2
   :caption: Getting Started:

   installation
   examples
   tartanground
   usage
   modalities
   environments
   troubleshooting
   
License
==========================================================

he TartanAir V2 dataset is licensed under a `Creative Commons Attribution 4.0 International License <https://creativecommons.org/licenses/by/4.0/>`_ and the toolkit is licensed under a `MIT License <https://opensource.org/license/mit>`_.
